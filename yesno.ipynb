{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C'est oui ou bien c'est non \n",
    "\n",
    "L'objectif de ce travail est de se servir d'un réseau de neurones pour réussir à classifier correctement un \"Oui\" et un \"Non\" prononcés à l'oral. Le thème de ce travail est inspiré par la chanson d'Angèle **Oui ou non**. \n",
    "\n",
    "# I-Construction du dataset\n",
    "\n",
    "On commence par importer des librairies utiles et par mettre en place l'environnement de travail. \n",
    "\n",
    "Ensuite, l'idée est d'extraire des propriétés pertinentes pour les fichiers audio afin de les numériser et d'être en mesure d'entraîner des modèles à partir de celles-ci. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "yes_audio_dir = './YesNo/yes/'\n",
    "no_audio_dir = './YesNo/no/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée une fonction pour justement extraire ces \"features\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an audio file\n",
    "def extract_features(audio_file, max_length=174):\n",
    "    # Read the audio file\n",
    "    sampling_rate, audio_data = wavfile.read(audio_file)\n",
    "    \n",
    "    # Ensure the audio is mono (if stereo, take the first channel)\n",
    "    if len(audio_data.shape) > 1:\n",
    "        audio_data = audio_data[:, 0]\n",
    "\n",
    "    # Convert audio data to floating-point representation\n",
    "    audio_data = audio_data.astype(float)\n",
    "\n",
    "    # Extract MFCC features \n",
    "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=13)\n",
    "\n",
    "    # Pad or truncate MFCCs to a fixed length\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        pad_width = max_length - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    return mfcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On n'a plus qu'à utiliser notre fonction sur nos fichiers ! J'ai choisi 13 comme nombre de features, de manière assez arbitraire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Process \"yes\" audio files\n",
    "for filename in os.listdir(yes_audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file = os.path.join(yes_audio_dir, filename)\n",
    "        mfccs = extract_features(audio_file)\n",
    "        features.append(mfccs)\n",
    "        labels.append(1)  # 1 for \"yes\"\n",
    "\n",
    "# Process \"no\" audio files\n",
    "for filename in os.listdir(no_audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file = os.path.join(no_audio_dir, filename)\n",
    "        mfccs = extract_features(audio_file)\n",
    "        features.append(mfccs)\n",
    "        labels.append(0)  # 0 for \"no\"\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a nos données bien étiquetées. Penchons-nous maintenant sur le modèle à utiliser. \n",
    "\n",
    "# II-  Le modèle\n",
    "On va se servir de la librairie `keras` pour accéder aux modèles disponibles sur `Tensorflow`. \n",
    "\n",
    "Avant cela, essayons d'entraîner nous-même un modèle avec une architecture classique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7985, 13, 174)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 172, 32)           1280      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 86, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 84, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 42, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2688)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               344192    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 351,938\n",
      "Trainable params: 351,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_frames = 174\n",
    "num_mfcc = 13\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(32, 3, activation='relu', input_shape=(num_frames, num_mfcc)))  # Adjust input shape\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))  # Two output classes (\"yes\" and \"no\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if one-hot encoding labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfait, maintenant on peut préparer nos données pour l'entraînement et le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6388, 2262)\n",
      "X_valid shape: (1597, 2262)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reshape the data to two dimensions (flatten along the second and third dimensions)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X back to its original shape (if needed)\n",
    "# X = X.reshape(X.shape[0], num_frames, num_mfcc)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok on peut lancer l'entraînement !\n",
    "\n",
    "# III- L'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape our data\n",
    "X_train = X_train.reshape(X_train.shape[0], num_frames, num_mfcc)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], num_frames, num_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 - 2s - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.1587 - val_accuracy: 0.9631 - 2s/epoch - 9ms/step\n",
      "Epoch 2/10\n",
      "200/200 - 1s - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.1606 - val_accuracy: 0.9637 - 1s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "200/200 - 2s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.1776 - val_accuracy: 0.9643 - 2s/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "200/200 - 1s - loss: 0.0123 - accuracy: 0.9950 - val_loss: 0.2003 - val_accuracy: 0.9562 - 1s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "200/200 - 2s - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.1783 - val_accuracy: 0.9537 - 2s/epoch - 8ms/step\n",
      "Epoch 6/10\n",
      "200/200 - 2s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.1929 - val_accuracy: 0.9662 - 2s/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "200/200 - 2s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9631 - 2s/epoch - 8ms/step\n",
      "Epoch 8/10\n",
      "200/200 - 2s - loss: 3.7216e-04 - accuracy: 1.0000 - val_loss: 0.2247 - val_accuracy: 0.9643 - 2s/epoch - 8ms/step\n",
      "Epoch 9/10\n",
      "200/200 - 2s - loss: 1.5425e-04 - accuracy: 1.0000 - val_loss: 0.2286 - val_accuracy: 0.9656 - 2s/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "200/200 - 2s - loss: 1.1330e-04 - accuracy: 1.0000 - val_loss: 0.2346 - val_accuracy: 0.9649 - 2s/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  \n",
    "    batch_size=32,  # classic batch size\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=2  # 1 for progress updates, 0 for no updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant évaluer les performances du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1661\n",
      "Validation Accuracy: 95.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est tout à fait satisfaisant ! On pourrait essayer d'aller plus loin et d'utiliser de l'augmentation de données mais on a déjà des résultats très satisfaisants. \n",
    "\n",
    "# IV- Pour aller plus loin\n",
    "\n",
    "On peut tester plusieurs pistes :\n",
    "- il serait intéressant de simplement essayer d'entraîner sur les données de son. \n",
    "- on peut aussi essayer une méthode \"bourrine\" en extrayant plus de features. \n",
    "\n",
    "Testons la deuxième méthode, car elle est très rapide à mettre en place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an audio file\n",
    "def extract_features20(audio_file, max_length=174):\n",
    "    # Read the audio file\n",
    "    sampling_rate, audio_data = wavfile.read(audio_file)\n",
    "    \n",
    "    # Ensure the audio is mono (if stereo, take the first channel)\n",
    "    if len(audio_data.shape) > 1:\n",
    "        audio_data = audio_data[:, 0]\n",
    "\n",
    "    # Convert audio data to floating-point representation\n",
    "    audio_data = audio_data.astype(float)\n",
    "\n",
    "    # Extract MFCC features \n",
    "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=20)\n",
    "\n",
    "    # Pad or truncate MFCCs to a fixed length\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        pad_width = max_length - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# Create empty lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Process \"yes\" audio files\n",
    "for filename in os.listdir(yes_audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file = os.path.join(yes_audio_dir, filename)\n",
    "        mfccs = extract_features20(audio_file)\n",
    "        features.append(mfccs)\n",
    "        labels.append(1)  # 1 for \"yes\"\n",
    "\n",
    "# Process \"no\" audio files\n",
    "for filename in os.listdir(no_audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file = os.path.join(no_audio_dir, filename)\n",
    "        mfccs = extract_features20(audio_file)\n",
    "        features.append(mfccs)\n",
    "        labels.append(0)  # 0 for \"no\"\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réexecute simplement le code précédent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6388, 3480)\n",
      "X_valid shape: (1597, 3480)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to two dimensions (flatten along the second and third dimensions)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X back to its original shape (if needed)\n",
    "# X = X.reshape(X.shape[0], num_frames, num_mfcc)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_10 (Conv1D)          (None, 172, 32)           1952      \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 86, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 84, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 42, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 2688)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               344192    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 352,610\n",
      "Trainable params: 352,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "200/200 - 4s - loss: 0.2235 - accuracy: 0.9059 - val_loss: 0.1586 - val_accuracy: 0.9386 - 4s/epoch - 18ms/step\n",
      "Epoch 2/10\n",
      "200/200 - 2s - loss: 0.1198 - accuracy: 0.9546 - val_loss: 0.1319 - val_accuracy: 0.9480 - 2s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "200/200 - 2s - loss: 0.0830 - accuracy: 0.9663 - val_loss: 0.1230 - val_accuracy: 0.9555 - 2s/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "200/200 - 2s - loss: 0.0686 - accuracy: 0.9737 - val_loss: 0.1198 - val_accuracy: 0.9587 - 2s/epoch - 8ms/step\n",
      "Epoch 5/10\n",
      "200/200 - 2s - loss: 0.0519 - accuracy: 0.9817 - val_loss: 0.1194 - val_accuracy: 0.9624 - 2s/epoch - 8ms/step\n",
      "Epoch 6/10\n",
      "200/200 - 2s - loss: 0.0393 - accuracy: 0.9873 - val_loss: 0.1261 - val_accuracy: 0.9574 - 2s/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "200/200 - 2s - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.1377 - val_accuracy: 0.9593 - 2s/epoch - 8ms/step\n",
      "Epoch 8/10\n",
      "200/200 - 2s - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.1780 - val_accuracy: 0.9549 - 2s/epoch - 9ms/step\n",
      "Epoch 9/10\n",
      "200/200 - 2s - loss: 0.0237 - accuracy: 0.9911 - val_loss: 0.1813 - val_accuracy: 0.9480 - 2s/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "200/200 - 2s - loss: 0.0198 - accuracy: 0.9926 - val_loss: 0.1762 - val_accuracy: 0.9568 - 2s/epoch - 8ms/step\n",
      "Validation Loss: 0.1762\n",
      "Validation Accuracy: 95.68%\n"
     ]
    }
   ],
   "source": [
    "num_frames = 174  # You can adjust this if needed\n",
    "num_mfcc = 20    # Set to 20 for 20 MFCC features\n",
    "\n",
    "# Assuming you have already loaded your dataset into X (features) and y (labels)\n",
    "\n",
    "# Reshape the data to match the new number of MFCC features (20)\n",
    "X_train = X_train.reshape(X_train.shape[0], num_frames, num_mfcc)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], num_frames, num_mfcc)\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(32, 3, activation='relu', input_shape=(num_frames, num_mfcc)))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))  # Two output classes (\"yes\" and \"no\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if one-hot encoding labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  \n",
    "    batch_size=32,  \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=2  \n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Grâce à un Réseau de neurones convolutif à la structure très basique, on arrive facilement, en extrayant des features de fichiers sonores, à bien différencier un `Oui prononcé` d'un `Non prononcé`. Cependant, augmenter le nombre de features extraites n'a pas forcément d'impact sur la performance du modèle (au-delà d'un certain stade typiquement). \n",
    "\n",
    "On pourrait encore tester d'autres pistes, mais j'ai réalisé ce travail pour servir d'introduction au sujet des modèles d'analyse de fichiers sonores. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
